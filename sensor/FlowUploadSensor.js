/*    Copyright 2016 Firewalla LLC
 *
 *    This program is free software: you can redistribute it and/or  modify
 *    it under the terms of the GNU Affero General Public License, version 3,
 *    as published by the Free Software Foundation.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU Affero General Public License for more details.
 *
 *    You should have received a copy of the GNU Affero General Public License
 *    along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
'use strict'

let util = require('util')
let async = require('asyncawait/async')
let await = require('asyncawait/await')
let zlib = require('zlib')
let Promise = require('bluebird')

let log = require('../net2/logger.js')(__filename)
let Sensor = require('./Sensor.js').Sensor
let SysManager = require('../net2/SysManager')
let sysManager = new SysManager()
let HostManager = require('../net2/HostManager.js')
let hostManager = new HostManager('cli', 'server')
let HostTool = require('../net2/HostTool')
let hostTool = new HostTool()
let flowTool = require('../net2/FlowTool')()
let flowUtil = require('../net2/FlowUtil')
let Bone = require('../lib/Bone.js')

let INTERVAL_MIN = 10 //10 seconds
let INTERVAL_MAX = 3600 //1 hour
let INTERVAL_DEFAULT = 900 //15 minutes
let MAX_FLOWS = 50000 //can upload at most 50000 flows(after aggregation) to cloud, about 20 mb after compress
let TIME_OFFSET = 90 //90 seconds for other process to store latest data into redis

class FlowUploadSensor extends Sensor {
    constructor() {
        super()
        
    }

    run() {
        this.validateConfig()
        log.info(JSON.stringify(this.config))
        this.startTime = new Date() / 1000 - this.config.offset
        setInterval(() => {
            this.schedule();
          }, this.config.interval * 1000)
    }

    validateConfig(){
        if (this.config == null) {
            this.config = {}
        }
        if (this.config.interval == null) {
            this.config.interval = INTERVAL_DEFAULT
        } else if (this.config.interval < INTERVAL_MIN) {
            this.config.interval = INTERVAL_MIN
        } else if (this.config.interval > INTERVAL_MAX) {
            this.config.interval = INTERVAL_MAX
        }

        if (this.config.maxFlows == null || this.config.maxFlows > MAX_FLOWS) {
            this.config.maxFlows = MAX_FLOWS
        }

        if (this.config.offset == null || this.config.offset < 0) {
            this.config.offset = TIME_OFFSET
        }
    }

    schedule() {
        let start = this.startTime
        let end = new Date() / 1000 - this.config.offset
        //upload flow to cloud
        log.info("try to upload flows from "
         + start + "(" + new Date(start * 1000).toUTCString() + ")" + 
         " to " + end + "(" + new Date(end * 1000).toUTCString() + ")")

         return async(() => {
            try {
                //set next start point
                this.startTime = end + 0.001
                
                let macs = hostManager.getActiveMACs()
                if (macs == null || macs.length == 0) {
                    log.info("host manager not ready, wait to next round")
                    return
                }
                let debug = sysManager.isSystemDebugOn()
                let flows = await(this.getAllFlows(macs, start, end, !debug))
                if (flows != null && flows.length > 0) {
                    let limitedFlows = this.limitFlows(flows)
                    limitedFlows.start = start
                    limitedFlows.end = end 

                  let data = JSON.stringify(limitedFlows)
                  log.debug("original:", data)
                  log.info("original length:" + data.length)
                    
                    let compressedData = await(this.compressData(data))
                  log.debug("compressed:", compressedData)
                  log.info("compressed length:" + compressedData.length)
                    log.info("compress ratio:" + data.length / compressedData.length)

                    this.uploadData(compressedData)
                } else {
                    log.info("empty flows, wait to next round")
                }
            } catch (err) {
                log.error("upload flows failed:" + err.toString())
            }
          })();
    }

    uploadData(data) {
        let toUpload = {
            payload : data
        }    
        Bone.flowgraph('flow', toUpload, function(err, response){
            if (err) {
                log.error("upload to cloud failed:" + err)
            } else {
                log.info("upload to cloud success:" + JSON.stringify(response))
            }
        })
    }

    compressData(data) {
        return async(() => {
            return new Promise(function (resolve, reject) {
                let input = new Buffer(data, 'utf8');
                zlib.deflate(input, (err, output) => {
                    if(err) {
                        reject(err)
                    } else {
                        resolve(output.toString('base64'))
                    }
                })
            })
        })();
    }

    limitFlows(flows) {
        //limit flows for upload
        let total = this.getSize(flows)
        var uploaded = total
        if (total > this.config.maxFlows) {
            log.info("number of flows(" + total + ") exceeded limit(" + this.config.maxFlows + "), need cut off")
            let avgLimit = this.config.maxFlows / flows.length
            //avgLimit means the number limit of flows for each mac
            flows.map(flow => {
                if (flow.flows.length > avgLimit) {
                    flow.flows = flow.flows.slice(0, avgLimit)
                }
            })
            uploaded = this.getSize(flows)
        }
        log.info("will upload " + uploaded + " flows")
        return {
            flows : flows,
            total : total,
            uploaded : uploaded
        }
    }

    getAllFlows(macs, start, end, needHash) {
        return async(() => {
            let flows = []
            macs.forEach((mac) => {
                let flow = await(this.getFlows(mac, start, end))
                if (flow != null && flow.length > 0) {
                    flows.push(
                      {
                        flows: this.processFlow(flow, needHash),
                        mac: needHash? flowUtil.hashMac(mac) : mac
                      }
                    )
                }
            })
            return flows
          })();
    }

    getSize(flows) {
        if (flows == null || flows.length == 0) {
            return 0
        } else {
            return flows.map(flow => flow.flows.length).reduce((a,b) => a + b)
        }
    }

    getFlows(mac, start, end) {
        return async(() => {
            let ips = await (hostTool.getIPsByMac(mac));
            let flows = [];
            ips.forEach((ip) => {
                let outgoingFlows = await (flowTool.queryFlows(ip, "in", start, end)); // in => outgoing
                flows.push.apply(flows, outgoingFlows);
                let incomingFlows = await (flowTool.queryFlows(ip, "out", start, end)); // out => incoming
                flows.push.apply(flows, incomingFlows);
            })
            return flows
        })()
    }

    aggregateFlows(flows) {
        /**
         * input flows sample:  18 fields in each object
         * [
         *  {
         *     "ts":"", start time
         *     "_ts":"", end time
         *     "sh":"", source host
         *     "dh":"", destination host
         *     "lh":"", local host
         *     "sp":"", source port
         *     "dp":"", destination port, deprecated by pf?
         *     "af":{}, application flow
         *     "flows":[]  flow details
         *     "pf":{}, destination port flows
         *     "bl":"" response body length?
         *     "ob":"" total orig bytes
         *     "rb":"" total response bytes
         *     "ct":"" count
         *     "lo":"" location
         *     "pr":"" protocol, deprecated by pf?
         *     "du":"" duration,
         *     "fd":"" direction, in/out
         *  }
         *  {...}
         *  {...}
         * ]
         * 
         *  aggregate by sh,dh,lh,fd, then copy "lo" at first level, and put the others to second nested array
         * 
         *  output flows sample: 5 fields in first level, 13 fields in second level
         * [
         *  {
         *     "sh":"",
         *     "dh":"",
         *     "lh":"",
         *     "fd":"",
         *     "lo":"",
         *     "agg":[
         *       {
         *         "ts":"",
         *         "_ts":"",
         *         "sp":"",
         *         "dp":"",
         *         "af":{},
         *         "pf":{},
         *         "flows":[],
         *         "bl":"",
         *         "ob":"",   
         *         "rb":"",
         *         "ct":"",
         *         "pr":"",
         *         "du":"",
         *       }
         *     ]
         *  },
         *  {...},
         *  {...}
         * ]
         * */

        let aggs = {} 
        for(var i = 0; i < flows.length; i++) {
            let flow = flows[i]
            let key = flow.sh + "," + flow.dh + "," + flow.lh + "," + flow.fd
            if (aggs[key] == null) {
                aggs[key] = {
                    sh : flow.sh,
                    dh : flow.dh,
                    fd : flow.fd,
                    lh : flow.lh,
                    lo : flow.lo,
                    agg : []
                }
            } 
            //merge other fields to agg
            let agg = {}
            let allFields = Object.keys(flow)
            allFields.forEach(function(f){
                if (f != 'sh' && f != 'dh' && f != 'lh' && f != 'lo' && f != 'fd') {
                    agg[f] = flow[f]
                }
            })
            aggs[key].agg.push(agg)
        }
        let result = Object.keys(aggs).map(k => aggs[k])
        log.info("size before agg:" + flows.length + ", after agg:" + result.length)
        return result
    }

    cleanFlow(flow) {
        //remove key with empty value
        Object.keys(flow).forEach(k => {
            if (flow[k] == null) {
                delete flow[k]
            } else if (Array.isArray(flow[k]) && flow[k].length == 0) {
                delete flow[k]
            } else if (typeof flow[k] === 'object' && Object.keys(flow[k]).length == 0) {
                delete flow[k]
            }
        })
        return flow
    }

    enrichFlow(flow) {
        //add location
        flowTool._enrichCountryInfo(flow)
        flow.lo = flow.country
        delete flow.country
        return flow
    }

    processFlow(flows, needHash) {
        return this.aggregateFlows(flows.map(flow => {

            //enrich
            this.enrichFlow(flow)

            //clean
            this.cleanFlow(flow)

            //hash
            if (needHash) {
                return flowUtil.hashFlow(flow, true)
            } else {
                return flow
            }
        }))
    }
}

module.exports = FlowUploadSensor
